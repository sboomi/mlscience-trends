{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe91925-fefc-447b-9d22-db8c862362f6",
   "metadata": {},
   "source": [
    "# PDF mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20efb219-77d5-4d69-b86d-1f6e0109deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pymongo\n",
    "import time\n",
    "\n",
    "# load_dotenv(find_dotenv)\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9520b106-3874-4b5b-8764-0998312823b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://ml4physicalsciences.github.io/\"\n",
    "\n",
    "data = {\n",
    "    \"title\": [],\n",
    "    \"authors\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022e5476-20be-44f7-93e4-a01318c6a38f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ml4physicalsciences.github.io/2017\n",
      "200\n",
      "https://ml4physicalsciences.github.io/2018\n",
      "404\n",
      "PAGE NOT FOUND\n",
      "https://ml4physicalsciences.github.io/2019\n",
      "200\n",
      "https://ml4physicalsciences.github.io/2020\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adversarial learning to eliminate systematic e...</td>\n",
       "      <td>Victor Estrade, Cecile Germain, Isabelle Guyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Variational Inference over Non-differentiable ...</td>\n",
       "      <td>Adam McCarthy, Blanca Rodriguez and Ana Minchole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep topology classifiers for a more efficient...</td>\n",
       "      <td>Daniel Weitekamp III, Thong Q. Nguyen, Dustin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nanophotonic Particle Simulation and Inverse D...</td>\n",
       "      <td>John Peurifoy, Yichen Shen, Li Jing, Yi Yang, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FlareNet: A Deep Learning Framework for Solar ...</td>\n",
       "      <td>Sean McGregor, Dattaraj Dhuri, Anamaria Berea ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Adversarial learning to eliminate systematic e...   \n",
       "1  Variational Inference over Non-differentiable ...   \n",
       "2  Deep topology classifiers for a more efficient...   \n",
       "3  Nanophotonic Particle Simulation and Inverse D...   \n",
       "4  FlareNet: A Deep Learning Framework for Solar ...   \n",
       "\n",
       "                                             authors  \n",
       "0  Victor Estrade, Cecile Germain, Isabelle Guyon...  \n",
       "1   Adam McCarthy, Blanca Rodriguez and Ana Minchole  \n",
       "2  Daniel Weitekamp III, Thong Q. Nguyen, Dustin ...  \n",
       "3  John Peurifoy, Yichen Shen, Li Jing, Yi Yang, ...  \n",
       "4  Sean McGregor, Dattaraj Dhuri, Anamaria Berea ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for y in range(2017,2021):\n",
    "    print(f\"{base_url}{y}\")\n",
    "    url = f\"{base_url}{y}\"\n",
    "    r = requests.get(url)\n",
    "    print(r.status_code)\n",
    "    if r.status_code == 200:\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        html_table = soup.select_one(\"section#papers\").select_one(\"div.table-wrapper\")\n",
    "        for el in html_table.select(\"td\"):\n",
    "            if not el.text.isdigit():\n",
    "                title, *_, authors = re.split(r'\\[(pdf|poster|video)\\]', el.text.strip())\n",
    "                data[\"title\"].append(title.strip())\n",
    "                data[\"authors\"].append(authors.strip())\n",
    "    else:\n",
    "        print(\"PAGE NOT FOUND\")\n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03088936-d590-4715-8d34-ae355dfbde17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cb0xi51dUWja8ZkNmegJYFvt64Dslznq'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get(\"CORE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2055256d-f57c-40f5-9317-33b0ad099126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational Inference over Non-differentiable Cardiac Simulators using Bayesian Optimization\n",
      "Adam McCarthy, Blanca Rodriguez and Ana Minchole\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1].title)\n",
    "print(df.iloc[1].authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1437dfda-9a08-4bad-ac7c-d1d01702c277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "API_KEY = os.environ.get(\"CORE_API_KEY\")\n",
    "q_params = {'apiKey': API_KEY}\n",
    "query=\"Variational%20Inference%20over%20Non-differentiable%20Cardiac%20Simulators%20using%20Bayesian%20Optimization%20Adam%20McCarthy%2C%20Blanca%20Rodriguez%20and%20Ana%20Minchole\"\n",
    "r = requests.get(f\"https://core.ac.uk:443/api-v2/search/{query}\", params=q_params)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c379c83-e82c-49f4-b86a-89697c96e451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'articles_2021_04_23',\n",
       " '_type': 'article',\n",
       " '_id': '141537424',\n",
       " '_score': 93.81691,\n",
       " '_source': {'id': '141537424',\n",
       "  'authors': ['McCarthy, Adam', 'Rodriguez, Blanca', 'Minchole, Ana'],\n",
       "  'citations': [],\n",
       "  'contributors': [],\n",
       "  'datePublished': '2017-12-09T00:00:00',\n",
       "  'deleted': 'ALLOWED',\n",
       "  'description': 'Performing inference over simulators is generally intractable as their\\nruntime means we cannot compute a marginal likelihood. We develop a\\nlikelihood-free inference method to infer parameters for a cardiac simulator,\\nwhich replicates electrical flow through the heart to the body surface. We\\nimprove the fit of a state-of-the-art simulator to an electrocardiogram (ECG)\\nrecorded from a real patient.Comment: Workshops on Deep Learning for Physical Sciences and Machine Learning\\n  4 Health, NIPS 201',\n",
       "  'fullText': 'Variational Inference over Non-differentiable Cardiac\\nSimulators using Bayesian Optimization\\nAdam McCarthy1, Blanca Rodriguez1, and Ana Mincholé1\\n1 Department of Computer Science, University of Oxford∗\\nAbstract\\nPerforming inference over simulators is generally intractable as their runtime means\\nwe cannot compute a marginal likelihood. We develop a likelihood-free inference\\nmethod to infer parameters for a cardiac simulator, which replicates electrical flow\\nthrough the heart to the body surface. We improve the fit of a state-of-the-art\\nsimulator to an electrocardiogram (ECG) recorded from a real patient.\\n1 Introduction\\nThe heart contracts due to the propagation of electricity through the heart wall. This electricity then\\npropagates through the torso and we can record an electrocardiogram (ECG). Cardiac simulators\\nreplicate this propagation, outputting an ECG mirroring that recorded from a real patient (see\\nCardone-Noott et al., 2016, for a review). They are slow, require significant compute power -\\ngenerally supercomputers, and are non-differentiable (we cannot compute its jacobian analytically).\\nWe must fit the parameters of these simulators efficiently so that they replicate the recording taken\\nfrom real patients and provide clinicians with deep, interpretable understandings of the sources of\\nrisk. These insights also aid basic research and the development of new treatments.\\nThis combination of slow run time and lack of differentiability make parameter inference and\\nuncertainty propagation significant challenges and active areas of research. We cannot compute a\\nlikelihood due to the simulator run time and these problems frequently have many minima.\\nAn approach to parameter inference is variational inference, which provides a parametric approx-\\nimation of the posterior of a probabilistic model. Early work on variational inference was limited\\nto forms in which gradient descent steps could be computed analytically (e.g. Jordan et al., 1999).\\nNowadays it is possible to take steps where the closed form is not known, for example using the\\nreparameterisation trick (Kingma and Welling, 2014). This uses a Monte Carlo estimate of the\\nlikelihood to approximate the gradient of the optimisation function, but requires that the simulator is\\ndifferentiable. Ranganath et al. (2014) drop this requirement with the score function estimate but\\ngradient estimates are noisy. We focus on the optimiser instead of the gradients by using Bayesian\\noptimisation, which being a global optimisation method makes converging to local minima less likely.\\nThis problem of parameter inference over non-differentiable simulators applies to many fields in\\nwhich simulators are used. We need to be able to fit weather simulators to predict hurricanes\\n(Rappaport et al., 2009), simulators of the formation of the universe (Boylan-Kolchin, 2014), and\\nsimulators to predict the spread of wildland fires (Sullivan, 2009), to give a few examples. We also\\nneed to understand how changes in these parameters affect the output, and how certain the simulators\\nare that events will happen.\\nContribution We develop a method to infer parameters for a non-differentiable simulator using\\nVariational Inference combined with Bayesian Optimisation, allowing us to fit a posterior distribution\\nwith a minimal number of trials and avoid local minima where possible. We show that we can use the\\ninferred parameters to improve the fit of a state-of-the-art cardiac simulator.\\n∗firstname.lastname@cs.ox.ac.uk\\nWorkshops on Deep Learning for Physical Sciences and Machine Learning 4 Health, NIPS 2017.\\nar\\nX\\niv\\n:1\\n71\\n2.\\n03\\n35\\n3v\\n1 \\n [s\\ntat\\n.M\\nL]\\n  9\\n D\\nec\\n 20\\n17\\nθ h ECG\\nFigure 1: The forward problem of the ECG involves taking the anatomy as a prior and defining\\nparameters representing biological features, then propagating electricity through the heart, diffusing\\nthe charge through the torso and producing a simulated ECG. The inverse problem involves performing\\ninference over the forward problem. θ are the model parameters, h is a heart simulator, and ECG is\\nthe output ECG trace.\\n2 Methodology\\nSimulator The state-of-the-art cardiac simulator implements the bidomain equations (see\\nTrayanova, 2011, for a review). The bidomain-based simulator, whilst physiologically very ac-\\ncurate, takes several hours to run on a supercomputer. We developed a fast cardiac simulator which\\nincludes an anatomical heart model derived from MRI as a prior, simulates electrical propagation\\nthrough the heart to the body surface, and outputs a 12-lead ECG (see Figure1 for a visualisation).\\nWhilst this simulator is much faster, it is still too slow to compute a likelihood. It has a 17 dimensional\\nparameter space consisting of locations at which electricity enters the heart wall and propagation\\nvelocities through the inner and outer heart wall, which have previously been measured in-vivo. For a\\nbiophysical meaning of these parameters, we refer the reader to Cardone-Noott et al. (2016).\\nThe simulator we use will be presented in future work, so for the purposes of this paper can be a\\nfunction f(θ) which has parameters θ ∈ Rp and returns output X ∈ Rd. It is a deterministic, black\\nbox function, with no simple closed form, and that it can be evaluated at any θ within the domain of\\ninterest. We have observations ECG ∈ Rd recorded from the patient and the aim is to compute a\\nposterior p(θ|ECG). We also test the inferred parameters on the bidomain-based simulator, which\\ncan be defined similarly, to demonstrate that we have improved the fit of the state-of-the-art.\\nVariational inference Given that our simulator is non-differentiable we cannot use methods which\\nuse second-order information (e.g. Regier et al., 2017) or reduce the variance of the gradients\\n(e.g. Miller et al., 2017; Roeder et al., 2017). We construct our variational objective following the\\nvariational autoencoder (Kingma and Welling, 2014) and optimise over the marginals of our Gaussian\\napproximating distribution: µ and σ. Variational autoencoders define a parametric generative model\\nwith likelihood p(X|θ) for each dimension, prior p(θ) over the parameters and a model q(θ|X) which\\napproximates the posterior. It can be shown that\\nlog p(x) ≥ −KL (q(θ|X) , p(θ)) + Eq(θ|X) [log p(X|θ)] (1)\\nwhere the right hand term in Equation 1 is termed the variational lower bound or evidence based\\nlower bound objective (ELBO). In variational inference we maximise the ELBO by optimising over\\nthe marginal parameters of Q.\\nOur approach shares many similarities with the variational autoencoder, but the neural network is\\nreplaced by a cardiac simulator. We avoid more recent work on improving the posterior distributions\\n(e.g. Ranganath et al., 2016; Liu and Wang, 2016; Huszár, 2017) and we will explain the reasons for\\nthis in Section 3.\\nOptimisation of the variational objective Our simulator is non-differentiable, so we cannot\\nperform gradient-based optimisation without estimating gradients in some way. We could approximate\\nthe jacobian using finite differences, but this requires n evaluations of the model, where n is the\\nnumber of dimensions over which we are optimising. There is also work on approximating gradients\\nin variational inference, but they tend to be noisy. Instead, we perform global optimisation using\\nBayesian Optimisation to make our method applicable to all non-differentiable simulators.\\nIt is common with local optimisation to grow the KL-divergence over time such that the log likelihood,\\nwhich fits the mean of Q, dominates initially. Global optimisation routines have memory, however, so\\nwe fit the log likelihood first and then the variance via the KL-divergence as independent optimisations\\nto prevent the KL-divergence dominating initially.\\n2\\nWhen performing Bayesian optimisation, we found a Square Exponential kernel was optimal, likely\\nbecause the optimisation surface should be relatively smooth. We initially used Expected Improve-\\nment as our acquisition function, but found that the noisy samples meant Augmented EI (Huang\\net al., 2006) had superior performance. We sample our initial design matrix using Latin Hypercube\\nSampling and define our parameter space θ as the endocardial conductivities representing the Purkinje\\nsystem, three conductivities for anisotropic conductivity in the myo- and epi-cardium, and the position\\nof the stimulus locations, which includes all of the nodes in the inner heart wall (the endocardium).\\nWe allow the conductivities to vary ±50%. For a biophysical meaning of these parameters we\\nrefer the reader to Trayanova (2011). We set bounds on the parameters to represent the underlying\\nphysiological variability (Britton et al., 2013), and define inequality constraints for:\\n1. The order of three of the conductivities representing the direction of muscle fibres. We know\\na priori that they are ordered.\\n2. The stimulus positions are position invariant, i.e. a, b, c ≡ c, b, a, so we specify that the first\\ndimension of the latent space representing the positions on a manifold through the heart\\n(which we will discuss in the following paragraph) is ordered.\\nDimensionality reduction So far we have assumed that our simulator is a black box. In this\\nsection, we move to a white box setting and use our knowledge of its internal dynamics to reduce\\ndimensionality. We take magnetic resonance imaging (MRI) scans of the patient’s heart and use this\\nto derive an anatomical mesh. When we use this mesh within the simulator by converting it to a\\ngraph representation of electrical propagation, we are enforcing a strong prior on its dynamics. The\\nheart itself lies on a lower dimensional manifold within Cartesian space due to the hollow ventricles,\\nwhich makes optimisation over Cartesian coordinates discontinuous. We reduce the dimensionality\\nof the graph vertices using an isomap (Tenenbaum et al., 2000). This works by computing geodesic\\ndistances between the vertices, and applying PCA to produce an embedding. We then use this\\nembedding as a 2D manifold over which to optimise and will show in Section 3 that this significantly\\nimproves convergence time. Bayesian optimisation can struggle in large dimensions because the\\nacquisition function, Expected Improvement, becomes flat near minima. We found that reducing the\\ndimensionality improved the accuracy of the inferred parameters.\\nIsomaps are injective, meaning there is no inverse mapping from the 2D latent space back into\\nCartesian space. There are, however, a finite number of nodes in the mesh. We compute a lookup\\ntable between the nodes in Cartesian space and those in the latent space. We use a kd-tree to take a\\nnode in latent space and find its nearest neighbour in Cartesian space using the lookup table. This is\\nnecessary to transfer parameters inferred to the bidomain-based simulator, which uses a finer mesh.\\n3 Results and discussion\\nIn this section we will report our results and then discuss how they improve fitting these simulators.\\nDimensionality reduction Figure 2a shows the heart mesh reduced to 2D using an isomap over\\n16 neighbours for each point, which produces a mean reconstruction error of only 0.0016mm when\\nprojecting back into Cartesian space. Reducing dimensionality from three to two will have a minimal\\neffect on the speed of fitting of a Gaussian Process, so the improvement in convergence speed and\\nperformance (see Figure 2b) is likely related to the increased smoothness of the optimisation surface\\ncreated by following the manifold.\\nParameter fitting Figure 3 shows an ECG recorded from a real patient and the associated bidomain\\nsimulation - the current state-of-the-art. Whilst the bidomain equations are physiologically accurate,\\ntheir long runtime makes parameter fitting difficult. We take the parameters inferred on the simpler\\nsimulator and show that they improve the fit of the bidomain-based simulator which is considered the\\ngold standard in cardiac modelling.\\nStructural modelling of the anatomy enforces a strong prior on the model which is essential in a small\\ndata regime such as medicine. Rather than treating the inverse problem as one of regression (e.g.\\nIntini et al., 2005), we perform inference over a simulator representing the forward problem which\\nprovides this prior.\\n3\\nIn-vivo measurements vary widely in the literature, due to noisy biological readings. Whilst maintain-\\ning uncertainty is important, it should be regarded with some scepticism due to the inaccuracy of the\\nunderlying readings on which these models are based. Choosing simple approximating distributions\\nis important for two reasons: firstly they simplify the problem, allowing us to fit based on limited data\\nand secondly they provide simpler, actionable decision support to clinicians. Projecting a Gaussian\\nonto the heart surface is far more interpretable for a surgeon, for example, than a more complex\\ndistribution which might span multiple parts of the heart. Enforcing simplicity is an important\\nconsideration in the medical domain.\\nFinally, simulators of this form often present many local minima, so whereas variational inference\\nusually relies on forms of stochastic gradient descent, we have used global optimisation which is\\nmore effective for providing accurate effective decision support.\\nRelated work Approximate Bayesian Computation (ABC) is a form of likelihood-free inference\\ncommonly used for these forms of problem. Tavaré et al. (1997) were the first to apply ABC, but it\\nwas named later by Beaumont et al. (2002). The simplest form of ABC, rejection ABC, is inefficient\\nbecause θ is arbitrarily sampled from its prior distribution. leading to numerate rejections. MCMC\\nABC builds a markov chain through the prior, creating a proposal distribution from which to obtain\\nnew query points (see Marjoram et al., 2003, for a review). Whilst this is more efficient than rejection\\nABC, it would still be intractable in our case due to the number of samples required.\\nPapamakarios and Murray (2016) avoid rejecting samples by directly approximating the posterior\\nas a mixture of Gaussians: a proposal prior q(θ). They draw samples from this prior and run the\\ncorresponding simulations, iteratively updating the proposal prior using a mixture density network\\n(MDN) (Bishop, 1994), until it matches the target posterior.\\nAnother approach is to approximate the likelihood function using a parametric model rather than\\navoiding its computation entirely, a synthetic likelihood. Wood (2010) used a single Gaussian\\ndistribution and Fan et al. (2012) used a mixture of Gaussians by learning multiple models based\\non repeated evaluations for fixed values of θ. Meeds and Welling (2014) used a Gaussian process to\\ncombine the likelihood approximations for each θ. Finally, we note a relationship with Louppe and\\nCranmer (2017), which has very similar motivations but relies on local optimisation using Variational\\nInference combined with a GAN.\\nIn the modelling literature, van Dam et al. (2016) provide a method for non-invasive mapping of the\\nheart from the 12-lead ECG. They enforce a strong prior by using a cardiac simulator as we do, but\\nuse local optimisation which may fall into local minima, whereas we use global optimisation. They\\nfind a point estimate of the PVC origin, whereas we provide a distribution over possible PVC origins.\\nThis is useful clinically, because it gives the clinician a measure of how accurate the model believes\\nthe localisation to be.\\nLastly, Giffard-Roisin et al. (2017) approach the problem by learning a regression. They also provide\\nuncertainty estimates and perform dimensionality reduction. In contrast to our method, they use\\n205 sensors across the torso (ECGI body surface potential maps Ramanathan et al., 2004), whereas\\nwe take standard 12-lead ECG as input. We can use less input data because the simulator enforces\\na strong prior. Their optimisation is local whereas our is global and they use Relevance Vector\\nMachines, which are patented by Microsoft, so this may be a consideration when applying their work.\\n4 Conclusion\\nWe have demonstrated an efficient, generic method for parameter inference over cardiac simulators\\nby using Variational Inference and Bayesian Optimization. We have used a cardiac simulator to\\ninfer parameters to reproduce the ECG of a patient and used these parameters to improve the fit of a\\nsimulator based on the gold standard for cardiac modelling: the bidomain equations.\\nThe ability to do efficient parameter inference makes it possible to personalise simulators and\\ncapture much more broadly the underlying electrophysiological variability. We plan to use this work\\nto improve our cardiac simulators by capturing new forms of variability, which will represent a\\nsignificant improvement in the accuracy of these simulators. We hope researchers from other fields\\nwill find similar benefits.\\n4\\nAcknowledgements Adam McCarthy is supported by EPSRC studentship OUCL/2016/AM.\\nBlanca Rodriguez (BR) and Ana Mincholé are supported by BR’s Wellcome Trust Senior Re-\\nsearch Fellowship in Basic Biomedical Sciences, the CompBiomed project (grant agreement No\\n675451) and the NC3R Infrastructure for Impact award (NC/P001076/1). Compute infrastructure\\nwas provided by the UK National Supercomputing Service (ARCHER Leadership Award e462) and\\na Microsoft Azure Research Award. We wish to thank Michael Osborne, Nando de Freitas, and\\nMatthew Graham for helpful discussions, and Andrew Trask for a thorough proofread of our paper.\\n0 500 1000 1500 2000\\nEvaluations\\n0.025\\n0.050\\n0.075\\n0.100\\n0.125\\n0.150\\n0.175\\n0.200\\n0.225\\nM\\nSE\\nIsomap\\nCartesian\\n15 10 5 0 5 10 158\\n6\\n4\\n2\\n0\\n2\\n4\\n6\\n8\\na) MSE cost curves for optimising over cartesian\\nspace and a manifold derived from an embedding\\nof the anatomical mesh using an isomap\\nb) A 2D embedding of the inner heart wall, the\\nendocardium, using an isomap over 16\\nneighbours\\nFigure 2: Plots to demonstrate dimensionality reduction\\nRecorded ECG Original fit Improved fit\\nFigure 3: Comparison between the recording from the real patient, the baseline simulation - the\\ncurrent state-of-the-art, and our improved fit. Our inference method allowed us to find parameters\\nwhich better fit the dynamics of the real patient’s heart. The recorded ECG is subject to electrical\\nnoise and noise from lead placement.\\n5\\nReferences\\nM. A. Beaumont, W. Zhang, and D. J. Balding. Approximate Bayesian Computation in Population Genetics.\\nGenetics, 162(4):2025–2035, 2002. ISSN 00166731. doi: GeneticsDecember1,2002vol.162no.42025-2035.\\nC. M. Bishop. Mixture density networks. Technical report, Aston University, 1994.\\nM. Boylan-Kolchin. Cosmology: A virtual Universe. Nature, 509(7499):170–171, 2014. ISSN 0028-0836. doi:\\n10.1038/509170a.\\nO. J. Britton, A. Bueno-Orovio, K. Van Ammel, H. R. Lu, R. Towart, D. J. Gallacher, and B. Rodriguez.\\nExperimentally calibrated population of models predicts and explains intersubject variability in cardiac\\ncellular electrophysiology. Proceedings of the National Academy of Sciences, 110(23):E2098–E2105, 2013.\\nISSN 0027-8424. doi: 10.1073/pnas.1304382110.\\nL. Cardone-Noott, A. Bueno-Orovio, A. Mincholé, N. Zemzemi, and B. Rodriguez. Human ventricular activation\\nsequence and the simulation of the electrocardiographic QRS complex and its variability in healthy and\\nintraventricular block conditions. Europace, 18(suppl_4):iv4–iv15, dec 2016. doi: 10.1093/europace/euw346.\\nY. Fan, D. J. Nott, and S. A. Sisson. Approximate Bayesian Computation via Regression Density Estimation.\\nStatistics, pages 1–17, 2012.\\nS. Giffard-Roisin, H. Delingette, T. Jackson, L. Fovargue, J. Lee, A. Rinaldi, N. Ayache, R. Razavi, and\\nM. Sermesant. Sparse Bayesian Non-linear Regression for Multiple Onsets Estimation in Non-invasive\\nCardiac Electrophysiology. In Functional Imaging and Modelling of the Heart, 2017.\\nD. Huang, T. T. Allen, W. I. Notz, and N. Zeng. Global optimization of stochastic black-box systems via\\nsequential kriging meta-models. Journal of Global Optimization, 34(3):441–466, 2006. ISSN 09255001. doi:\\n10.1007/s10898-005-2454-3.\\nF. Huszár. Variational Inference using Implicit Distributions. arXiv:1702.08235, 2017.\\nA. Intini, R. N. Goldstein, P. Jia, C. Ramanathan, K. Ryu, B. Giannattasio, R. Gilkeson, B. S. Stambler,\\nP. Brugada, W. G. Stevenson, Y. Rudy, and A. L. Waldo. Electrocardiographic imaging (ECGI), a novel\\ndiagnostic modality used for mapping of focal left ventricular tachycardia in a young athlete. Heart Rhythm,\\n2(11):1250–1252, 2005. ISSN 15475271. doi: 10.1016/j.hrthm.2005.08.019.\\nM. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. Introduction to variational methods for graphical\\nmodels. Machine Learning, 37(2):183–233, 1999. ISSN 08856125. doi: 10.1023/A:1007665907178.\\nD. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. In International Conference on Learning\\nRepresentations, 2014.\\nQ. Liu and D. Wang. Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm.\\narXiv:1608.04471, aug 2016.\\nG. Louppe and K. Cranmer. Adversarial Variational Optimization of Non-Differentiable Simulators.\\narXiv:1707.07113, jul 2017.\\nP. Marjoram, J. Molitor, V. Plagnol, and S. Tavare. Markov chain Monte Carlo without likelihoods. Proceedings\\nof the National Academy of Sciences of the United States of America, 100(26):15324–8, dec 2003. ISSN\\n0027-8424. doi: 10.1073/pnas.0306899100.\\nE. Meeds and M. Welling. GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation. arXiv\\npreprint arXiv:1401.2838, pages 1–17, 2014. ISSN <null>.\\nA. C. Miller, N. J. Foti, A. D’Amour, and R. P. Adams. Reducing Reparameterization Gradient Variance. may\\n2017.\\nG. Papamakarios and I. Murray. Fast \\x0f-free Inference of Simulation Models with Bayesian Conditional Density\\nEstimation. In Advances in Neural Information Processing Systems, pages 1028–1036, 2016.\\nC. Ramanathan, R. N. Ghanem, P. Jia, K. Ryu, and Y. Rudy. Noninvasive electrocardiographic imaging for\\ncardiac electrophysiology and arrhythmia. Nature medicine, 10(4):422–428, 2004. ISSN 1078-8956. doi:\\n10.1038/nm1011.\\nR. Ranganath, S. Gerrish, and D. M. Blei. Black Box Variational Inference. AISTATS, 33:814–822, 2014. ISSN\\n15337928.\\n6\\nR. Ranganath, J. Altosaar, D. Tran, and D. M. Blei. Operator Variational Inference. In Advances in Neural\\nInformation Processing, oct 2016.\\nE. N. Rappaport, J. L. Franklin, L. a. Avila, S. R. Baig, J. L. Beven, E. S. Blake, C. a. Burr, J.-G. Jiing, C. a.\\nJuckins, R. D. Knabb, C. W. Landsea, M. Mainelli, M. Mayfield, C. J. McAdie, R. J. Pasch, C. Sisko,\\nS. R. Stewart, and A. N. Tribble. Advances and Challenges at the National Hurricane Center. Weather and\\nForecasting, 24(2):395–419, 2009. ISSN 0882-8156. doi: 10.1175/2008WAF2222128.1.\\nJ. Regier, M. I. Jordan, and J. McAuliffe. Fast Black-box Variational Inference through Stochastic Trust-Region\\nOptimization. In Advances in Neural Information Processing Systems, 2017.\\nG. Roeder, Y. Wu, and D. Duvenaud. Sticking the Landing: Simple, Lower-Variance Gradient Estimators for\\nVariational Inference. arXiv:1703.09194, 2017.\\nA. L. Sullivan. Wildland surface fire spread modelling, 1990–2007. 1: Physical and quasi-physical models.\\nInternational Journal of Wildland Fire, 18(4):349–368, 2009. ISSN 1049-8001. doi: 10.1071/WF06144.\\nS. Tavaré, D. J. Balding, R. C. Griffiths, and P. Donnelly. Inferring coalescence times from DNA sequence data,\\n1997. ISSN 00166731.\\nJ. B. Tenenbaum, V. de Silva, and J. C. Langford. A Global Geometric Framework for Nonlinear Dimensionality\\nReduction. Science, 290(5500):2319–2323, dec 2000. doi: 10.1126/science.290.5500.2319.\\nN. A. Trayanova. Whole-heart modeling : Applications to cardiac electrophysiology and electromechanics,\\n2011. ISSN 00097330.\\nP. M. van Dam, N. G. Boyle, M. M. Laks, and R. Tung. Localization of premature ventricular contractions from\\nthe papillary muscles using the standard 12-lead electrocardiogram: a feasibility study using a novel cardiac\\nisochrone positioning system. Europace, pii(5):euw099, 2016. doi: 10.1093/europace/euw347.\\nS. N. Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466(August):\\n1102–1104, 2010. ISSN 0028-0836. doi: 10.1038/nature09319.\\n7\\n',\n",
       "  'fullTextIdentifier': 'http://arxiv.org/abs/1712.03353',\n",
       "  'identifiers': ['oai:arXiv.org:1712.03353', None],\n",
       "  'journals': None,\n",
       "  'language': {'code': 'en', 'id': 9, 'name': 'English'},\n",
       "  'duplicateId': None,\n",
       "  'publisher': None,\n",
       "  'rawRecordXml': '<record><header><identifier>\\n\\n\\n oai:arXiv.org:1712.03353</identifier><datestamp>\\n 2017-12-12</datestamp><setSpec>\\n stat</setSpec>\\n</header><metadata><oai_dc:dc xmlns:oai_dc=\"http://www.openarchives.org/OAI/2.0/oai_dc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd\" ><dc:title>\\n\\n \\n Variational Inference over Non-differentiable Cardiac Simulators using\\n  Bayesian Optimization</dc:title><dc:creator>\\n McCarthy, Adam</dc:creator><dc:creator>\\n Rodriguez, Blanca</dc:creator><dc:creator>\\n Minchole, Ana</dc:creator><dc:subject>\\n Statistics - Machine Learning</dc:subject><dc:description>\\n   Performing inference over simulators is generally intractable as their\\nruntime means we cannot compute a marginal likelihood. We develop a\\nlikelihood-free inference method to infer parameters for a cardiac simulator,\\nwhich replicates electrical flow through the heart to the body surface. We\\nimprove the fit of a state-of-the-art simulator to an electrocardiogram (ECG)\\nrecorded from a real patient.\\n</dc:description><dc:description>\\n Comment: Workshops on Deep Learning for Physical Sciences and Machine Learning\\n  4 Health, NIPS 2017</dc:description><dc:date>\\n 2017-12-09</dc:date><dc:type>\\n text</dc:type><dc:identifier>\\n http://arxiv.org/abs/1712.03353</dc:identifier>\\n </oai_dc:dc>\\n</metadata>\\n</record>',\n",
       "  'relations': [],\n",
       "  'repositories': [{'id': '144',\n",
       "    'openDoarId': 0,\n",
       "    'name': 'arXiv.org e-Print Archive',\n",
       "    'urlHomepage': None,\n",
       "    'urlOaipmh': None,\n",
       "    'uriJournals': None,\n",
       "    'physicalName': 'noname',\n",
       "    'source': None,\n",
       "    'software': None,\n",
       "    'metadataFormat': None,\n",
       "    'description': None,\n",
       "    'journal': None,\n",
       "    'roarId': 0,\n",
       "    'baseId': 0,\n",
       "    'pdfStatus': None,\n",
       "    'nrUpdates': 0,\n",
       "    'disabled': False,\n",
       "    'lastUpdateTime': None,\n",
       "    'repositoryLocation': None}],\n",
       "  'repositoryDocument': {'pdfStatus': 1,\n",
       "   'textStatus': 1,\n",
       "   'metadataAdded': 1513329032000,\n",
       "   'metadataUpdated': 1608819842000,\n",
       "   'timestamp': 1513036800000,\n",
       "   'depositedDate': 1513036800000,\n",
       "   'indexed': 1,\n",
       "   'deletedStatus': '0',\n",
       "   'pdfSize': 414053,\n",
       "   'tdmOnly': True,\n",
       "   'pdfOrigin': 'http://arxiv.org/abs/1712.03353'},\n",
       "  'similarities': None,\n",
       "  'subjects': ['text'],\n",
       "  'title': 'Variational Inference over Non-differentiable Cardiac Simulators using\\n  Bayesian Optimization',\n",
       "  'topics': ['Statistics - Machine Learning'],\n",
       "  'types': [],\n",
       "  'urls': ['http://arxiv.org/abs/1712.03353'],\n",
       "  'year': 2017,\n",
       "  'doi': None,\n",
       "  'oai': 'oai:arXiv.org:1712.03353',\n",
       "  'downloadUrl': 'http://arxiv.org/abs/1712.03353',\n",
       "  'pdfHashValue': '4e9f9ba86361d9c1b85cb3e6f454b7c99b4fa684',\n",
       "  'documentType': 'research',\n",
       "  'documentTypeConfidence': 1,\n",
       "  'citationCount': None,\n",
       "  'estimatedCitationCount': None,\n",
       "  'acceptedDate': None,\n",
       "  'depositedDate': 1513036800000,\n",
       "  'publishedDate': 1512777600000,\n",
       "  'issn': None,\n",
       "  'attachmentCount': 0,\n",
       "  'repositoryPublicReleaseDate': None,\n",
       "  'extendedMetadataAttributes': None,\n",
       "  'crossrefDocument': None,\n",
       "  'magDocument': None,\n",
       "  'orcidAuthors': None}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f80eff-b3b5-4927-9238-ae91812f44f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neurips', 'sample_airbnb', 'sample_analytics', 'sample_geospatial', 'sample_mflix', 'sample_restaurants', 'sample_supplies', 'sample_training', 'sample_weatherdata', 'admin', 'local']\n",
      "['ml4physics']\n"
     ]
    }
   ],
   "source": [
    "user = os.environ.get('MONGO_DB_USERNAME')\n",
    "password = os.environ.get('MONGO_DB_PASSWORD')\n",
    "\n",
    "client = pymongo.MongoClient(f\"mongodb+srv://{user}:{password}@maincluster.otbuf.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "\n",
    "print(client.list_database_names())\n",
    "db = client.neurips\n",
    "\n",
    "print(db.list_collection_names())\n",
    "\n",
    "collection = db.ml4physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eed3ad3-cc6b-4872-9eef-5f6bda050fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://papers.nips.cc/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEURIPS_URL = \"https://papers.nips.cc/\"\n",
    "NEURIPS_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9461e295-f00b-490a-8a0f-7771c380e922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02e74f10e0327ad868d138f2b4fdd6f0</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03afdbd66e7929b125f8597834fa83a4</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>072b030ba126b2f4b2374f342be9ed44</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>093f65e080a295f8076b1c5722a46aa2</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14bfa6bb14875e45bba028a21ed38046</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hash  year\n",
       "0  02e74f10e0327ad868d138f2b4fdd6f0  1987\n",
       "1  03afdbd66e7929b125f8597834fa83a4  1987\n",
       "2  072b030ba126b2f4b2374f342be9ed44  1987\n",
       "3  093f65e080a295f8076b1c5722a46aa2  1987\n",
       "4  14bfa6bb14875e45bba028a21ed38046  1987"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_url_hashs = {\n",
    "    \"hash\": [],\n",
    "    \"year\": []\n",
    "}\n",
    "\n",
    "for y in range(1987, 2021):\n",
    "    r = requests.get(f\"{NEURIPS_URL}paper/{y}\")\n",
    "    if r.status_code == 200:\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        for p in soup.select(\"div.col li a\"):\n",
    "            url = p['href']\n",
    "            *_, abstract = url.split(\"/\")\n",
    "            hash_url, *_ = abstract.split(\"-\")\n",
    "            paper_url_hashs[\"hash\"].append(hash_url)\n",
    "            paper_url_hashs[\"year\"].append(y)\n",
    "    else:\n",
    "        print(\"Something went wrong:\", r.status_code)\n",
    "            \n",
    "paper_refs = pd.DataFrame(paper_url_hashs)\n",
    "paper_refs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63164436-bfe9-40cb-a268-8d3a3f5a9ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Bit-Serial Neural Networks',\n",
       " 'book': 'Neural Information Processing Systems',\n",
       " 'page_first': 573,\n",
       " 'page_last': 583,\n",
       " 'abstract': None,\n",
       " 'full_text': '573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan F.  Murray,  Anthony V . W.  Smith  and Zoe F.  Butler. \\n\\nDepartment of Electrical Engineering,  University of Edinburgh, \\n\\nThe King\\'s Buildings, Mayfield Road,  Edinburgh, \\n\\nScotland,  EH93JL. \\n\\nABSTRACT \\n\\nA  bit  - serial  VLSI  neural  network  is  described  from  an  initial  architecture  for  a \\nsynapse array through to silicon layout and board design.  The issues surrounding bit \\n- serial  computation,  and  analog/digital  arithmetic  are  discussed  and  the  parallel \\ndevelopment  of  a  hybrid  analog/digital  neural  network  is  outlined.  Learning  and \\nrecall  capabilities  are  reported  for  the  bit  - serial  network  along  with  a  projected \\nspecification  for  a  64  - neuron,  bit  - serial  board  operating  at 20 MHz.  This tech(cid:173)\\nnique  is  extended  to  a  256  (2562  synapses)  network  with  an  update  time  of 3ms, \\nusing  a  \"paging\"  technique  to  time  - multiplex  calculations  through  the  synapse \\narray. \\n\\n1. INTRODUCTION \\n\\nThe functions a  synthetic neural network may aspire to mimic are the ability to con(cid:173)\\nsider  many  solutions  simultaneously,  an  ability  to  work  with  corrupted  data  and  a \\nnatural  fault  tolerance.  This  arises  from  the  parallelism  and  distributed  knowledge \\nrepresentation  which  gives  rise  to  gentle  degradation  as  faults  appear.  These func(cid:173)\\ntions  are  attractive  to implementation  in VLSI  and  WSI.  For example,  the natural \\nfault  - tolerance  could  be  useful  in  silicon  wafers  with  imperfect  yield,  where  the \\nnetwork  degradation  is  approximately  proportional  to  the  non-functioning  silicon \\narea. \\nTo cast  neural networks in engineering language,  a  neuron is a  state machine that is \\neither  \"on\"  or  \"off\\',  which  in  general  assumes  intermediate  states  as  it  switches \\nsmoothly  between  these  extrema.  The  synapses  weighting  the  signals  from  a \\ntransmitting neuron  such that it is more or less excitatory or inhibitory to the receiv(cid:173)\\ning  neuron.  The  set  of synaptic weights  determines  the stable  states and  represents \\nthe learned  information in a system. \\nThe  neural  state,  VI\\'  is  related  to  the  total  neural  activity  stimulated  by  inputs  to \\nthe  neuron  through  an  activation junction,  F.  Neural  activity  is  the  level  of excita(cid:173)\\ntion  of the  neuron  and the  activation  is  the way  it  reacts  in a  response to a  change \\nin activation. The neural output state at time t, V[,  is related to x[ by \\n\\nV[  = F (xf) \\n\\n(1) \\n\\nThe  activation  function  is  a  \"squashing\"  function  ensuring  that  (say)  Vi  is  1  when \\nXi  is large  and  -1  when Xi  is  small.  The neural update function  is therefore straight(cid:173)\\nforward: \\n\\n. \\n\\n,+1  - ,   + ~  ~ T  V\\' \\nJ \\nXI \\n\\ni-n-l \\n0  ~  ii \\n\\n- XI \\n\\n• •••• \\n\\nJ-O \\n\\n(2) \\n\\nwhere  8  represents  the  rate  of change  of neural  activity,  Tij \\nand n  is  the number of terms giving an n  - neuron array [1]. \\nAlthough  the  neural function  is  simple  enough,  in  a  totally  interconnected  n  - neu(cid:173)\\nron  network  there  are n 2  synapses requiring n 2  multiplications  and  summations and \\n\\nis  the  synaptic  weight \\n\\n© American Institute of Physics 1988 \\n\\n\\x0c574 \\n\\na large number of interconnects.  The challenge in VLSI is therefore to design a  sim(cid:173)\\nple,  compact  synapse  that  can  be  repeated  to  build  a  VLSI  neural  network  with \\nIn  a  network  with  fixed  functionality,  this  is  relatively \\nmanageable  interconnect. \\nstraightforward.  H the  network  is to be able to learn,  however,  the synaptic weights \\nmust  be programmable, and therefore more complicated. \\n\\n2. DESIGNING  A NEURAL  NETWORK IN  VLSI \\n\\nThere  are  fundamentally  two  approaches  to  implementing  any  function  in  silicon  -\\ndigital and analog.  Each technique has its advantages and  disadvantages,  and these \\nare  listed  below,  along  with  the  merits  and  demerits  of bit  - serial  architectures  in \\ndigital (synchronous) systems. \\nDigital  vs.  analog:  The  primary  advantage  of digital  design  for  a  synapse  array  is \\nthat  digital  memory  is  well  understood,  and  can  be  incorporated  easily.  Learning \\nnetworks are  therefore  possible  without  recourse  to unusual  techniques  or technolo(cid:173)\\ngies.  Other strengths of a digital approach are that design techniques are advanced, \\nautomated  and  well  understood  and  noise  immunity  and  computational  speed  can \\nbe  high.  Unattractive features  are  that  digital  circuits  of this complexity need  to  be \\nsynchronous  and  all  states  and  activities  are  quantised,  while  real  neural  networks \\nare  asynchronous  and  unquantised.  Furthermore,  digital  multipliers  occupy  a  large \\nsilicon  area, giving a low synapse count on  a single chip. \\nThe  advantages  of  analog  circuitry  are  that  asynchronous  behaviour  and  smooth \\nneural  activation  are  automatic.  Circuit  elements can  be  small,  but  noise  immunity \\nis relatively  low  and  arbitrarily  high  precision is not  possible.  Most  importantly,  no \\nreliable  analog,  non  - volatile  memory  technology  is  as  yet  readily  available.  For \\nthis  reason,  learning  networks  lend  themselves  more  naturally to  digital  design  and \\nimplementation. \\nSeveral  groups  are  developing  neural  chips  and  boards,  and  the  following  listing \\ndoes  not  pretend  to  be  exhaustive.  It is  included,  rather,  to indicate  the spread  of \\nactivity  in  this  field.  Analog  techniques  have  been  used  to  build  resistor  I  opera(cid:173)\\ntional  amplifier  networks [2,3]  similar to  those  proposed  by  Hopfield  and Tank [4]. \\nA  large  group  at  Caltech  is  developing  networks  implementing  early  vision  and \\nauditory  processing  functions  using the intrinsic nonlinearities of MaS transistors in \\nthe subthreshold  regime  [5,6].  The problem of implementing analog  networks with \\nelectrically  programmable  synapses  has  been  addressed  using  CCDIMNOS technol(cid:173)\\nogy  [7].  Finally,  Garth  [8]  is  developing  a  digital  neural  accelerator  board  (\"Net(cid:173)\\nsim\")  that  is  effectively  a  fast  SIMD  processor  with  supporting  memory  and  com(cid:173)\\nmunications chips. \\nBit - serial  vs.  bit  - parallel:  Bit  - serial  arithmetic and  communication  is  efficient \\nfor  computational  processes,  allowing  good  communication  within  and  between \\nVLSI  chips  and  tightly  pipelined  arithmetic  structures.  It  is  ideal  for  neural  net(cid:173)\\nworks  as  it  minimises  the  interconnect  requirement  by  eliminating  multi  - wire \\nbusses.  Although  a  bit  - parallel  design  would  be  free  from  computational  latency \\n(delay  between  input  and  output),  pipelining  makes  optimal  use  of  the  high  bit  -\\nrates possible in serial systems,  and  makes for  efficient circuit usage. \\n2.1  An asynchronous pulse stream VLSI neural network: \\nIn  addition  to  the  digital  system  that  forms  the  substance  of  this  paper,  we  are \\ndeveloping  a  hybrid  analOg/digital  network  family.  This work  is  outlined  here,  and \\nhas  been  reported  in  greater  detail  elsewhere  [9, 10, 11].  The  generic  (logical  and \\nlayout)  architecture  of a  single  network  of n  totally  interconnected neurons is  shown \\n\\n\\x0c575 \\n\\nschematically  in  figure  1.  Neurons  are  represented  by  circles,  which  signal  their \\nstates,  Vi  upward  into  a  matrix  of  synaptic  operators.  The  state  signals  are  con(cid:173)\\nnected  to  a  n  - bit  horizontal  bus  running  through  the  synaptic  array,  with  a  con(cid:173)\\nnection  to  each  synaptic  operator  in  every  column.  All  columns  have  n  operators \\n(denoted  by  squares)  and  each  operator adds its synaptic contribution,  Tij V j\\n,  to the \\nrunning  total  of  activity  for  the  neuron  i  at  the  foot  of  the  column.  The  synaptic \\nfunction  is  therefore  to  multiply  the  signalling  neuron  state,  Vj\\n,  by  the  synaptic \\nweight,  Tij ,  and  to  add  this  product  to  the  running  total.  This  architecture  is com(cid:173)\\nmon to both  the bit - serial and pulse - stream networks. \\n\\nSynapse \\n\\nStates { Vj  } \\n\\nFigure 1. Generic architecture for  a  network of n totally interconnected neurons. \\n\\nNeurons \\n\\nj=O \\n\\nj=II -1 \\n\\nThis type of architecture has many attractions for  implementation in 2  - dimensional \\nsilicon  as  the  summation  2  Tij Vj  is  distributed  in  space.  The  interconnect \\nrequirement  (n  inputs  to  each  neuron)  is  therefore  distributed  through  a  column, \\nreducing the need  for  long - range wiring.  The architecture is modular,  regular and \\ncan be easily expanded. \\nIn  the  hybrid  analog/digital  system,  the  circuitry  uses  a  \"pulse  stream\"  signalling \\nmethod  similar  to  that  in  a  natural  neural  system.  Neurons  indicate  their  state  by \\nthe  presence  or  absence  of  pulses  on  their  outputs,  and  synaptic  weighting  is \\nachieved  by  time  - chopping  the  presynaptic  pulse  stream  prior  to  adding  it  to  the \\npostsynaptic  activity  summation.  It  is  therefore  asynchronous  and  imposes  no fun(cid:173)\\ndamental  limitations  on  the  activation  or  neural  state.  Figure  2  shows  the  pulse \\nstream  mechanism  in  more  detail.  The synaptic  weight  is  stored  in  digital  memory \\nlocal to the operator.  Each synaptic operator has an  excitatory and inhibitory  pulse \\nstream  input  and  output.  The  resultant  product  of  a  synaptic  operation,  Tij Vj\\n,  is \\nadded  to  the  running  total  propagating  down  either  the  excitatory  or  inhibitory \\nchannel.  One binary bit  (the  MSBit)  of the  stored  Tij  determines whether  the con(cid:173)\\ntribution  is excitatory or inhibitory. \\nThe  incoming  excitatory  and  inhibitory  pulse  stream  inputs  to  a  neuron  are \\nintegrated  to  give  a  neural  activation  potential  that varies  smoothly  from  0  to  5  V. \\nThis  potential controls a  feedback  loop with  an odd number of logic  inversions and \\n\\n\\x0c576 \\n\\n. • • \\n\\nXT •• \\n\\nV , \\n.u.u, \\n• \\n\\nFigure  2.  Pulse  stream  arithmetic.  Neurons  are  denoted  by  0  and synaptic  operators \\nby  D. \\n\\nthus  forms  a  switched  \"ring - oscillator\".  H the inhibitory input dominates,  the feed(cid:173)\\nback  loop  is  broken.  H  excitatory  spikes  subsequently  dominate  at  the  input,  the \\nneural activity rises  to 5V and the feedback  loop oscillates with  a period determined \\nby a  delay  around  the loop.  The resultant  periodic waveform is then converted to a \\nseries  of voltage  spikes,  whose  pulse  rate  represents  the  neural  state,  Vi\\'  Interest(cid:173)\\ningly,  a  not  dissimilar  technique is  reported  elsewhere  in this volume,  although  the \\nsynapse function  is executed differently [12]. \\n\\n3. A 5  - STATE BIT - SERIAL NEURAL  NETWORK \\n\\nThe  overall  architecture  of  the  5  - state  bit  - serial  neural  network  is  identical  to \\nthat  of  the  pulse  stream  network.  It  is  an  array  of n 2  interconnected  synchronous \\nsynaptic  operators,  and  whereas  the  pulse  stream  method  allowed  Vj  to  assume  all \\nvalues  between  \"off\\' and  \"on\",  the  5 - state network VJ  is constrained  to 0,  ±0.5 Qr \\n± 1.  The resultant  activation  function  is  shown  in  Figure 3.  Full  digital  multiplica(cid:173)\\ntion  is  costly  in  silicon  area,  but  multiplication  of  Tij  by  Vj  =  0.5  merely  requires \\nthe synaptic  weight  to be right  - shifted  by  1 bit.  Similarly,  multiplication  by  0.25 \\ninvolves  a  further  right  - shift  of Til\\'  and  multiplication  by 0.0  is  trivially  easy.  VJ \\n<  0 is not  problematic,  as  a  switchable adder/subtractor  is  not much  more complex \\nthan  an  adder.  Five  neural  states  are  therefore  feasible  with  circuitry  that  is  only \\nslightly more complex  than  a  simple serial adder.  The neural state expands from a  1 \\nbit  to  a  3  bit  (5  - state)  representation,  where  the  bits  represent  \"add/subtract?\", \\n\"shift?\" and \"multiply by O?\". \\nFigure 4  shows  part of the synaptic  array.  Each synaptic operator includes an 8 bit \\nshift  register  memory  block  holding  the  synaptic  weight,  Til\\'  A  3  bit  bus  for  the  5 \\nneural  states  runs  horizontally  above  each  synaptic  row.  Single  phase  dynamic \\nCMOS  has  been  used  with  a  clock  frequency  in  excess  of 20  MHz  [13).  Details of \\na synaptic operator are  shown  in  figure 5.  The synaptic weight  Til  cycles around the \\nshift  register  and  the  neural  state  Vj  is  present  on  the  state  bus.  During  the  first \\nclock  CYCle,  the  synaptic  weight  is  multiplied  by  the  neural  state  and  during  the \\nsecond,  the  most  significant  bit (MSBit)  of the resultant  Tij Vj  is sign  - extended for \\n\\n\\x0c577 \\n\\nlHRESHOLD \\n\\nState VJ \\n\\n..... -------=-------.. Activity sJ \\n\\ns· \\n\\n\"5  STATE\" \\n\\n\"Sharper\" \\n\\n\"Smoother\" \\n\\n~.....::~-\"\\'--x.&..t------ Activity \"J \\n\\nFigure 3.  \"Hard - threshold\",  5  - state and sigmoid activation functions. \\n\\nJ-a-1T  v \\n~  ..  J \\nJ-li \\n\\nv, \\n\\nv, \\n\\nFigure 4.  Section  of the  synaptic  array  of the  5  - state activation function  neural net(cid:173)\\nwork. \\n\\n8  bits  to  allow  for  word  growth  in  the  running  summation.  A  least  significant  bit \\n(LSBit)  signal  running down  the  synaptic  columns indicates the arrival  of the LSBit \\nof  the  Xj  running  total.  If  the  neural  state  is  ±O.5  the  synaptic  weight  is  right \\nshifted  by  1 bit and then added to or subtracted from  the running total.  A  multipli(cid:173)\\ncation  of  ± 1  adds  or  subtracts  the  weight  from  the  total  and  multiplication  by  0 \\n\\n\\x0c578 \\n\\n.0.5 \\n.0.0 \\n\\nAdd/Subtract \\n\\nAdd! \\nSubtract \\n\\nCarry \\n\\nFigure S.  The  synaptic operator with a 5 - state activation function. \\n\\ndoes not alter the running summation. \\nThe  final  summation  at  the  foot  of the  column  is  thresholded  externally  according \\nto  the  5  - state activation function  in  figure  3.  As  the  neuron activity Xj\\'  increases \\nthrough  a  threshold  value  x\" \\nideal  sigmoidal  activation  represents  a  smooth  switch \\nof  neural  state  from  -1  to  1.  The 5  - state  \"staircase\"  function  gives a  superficially \\nmuch  better  approximation  to  the  sigmoid  form  than  a  (much  simpler  to  imple(cid:173)\\nment)  threshold  function.  The  sharpness  of  the  transition  can  be  controlled  to \\n\"tune\"  the  neural dynamics for  learning and computation.  The control parameter is \\nreferred  to  as  temperature  by  analogy  with  statistical  functions  with  this  sigmoidal \\nform.  High  \"temperature\" gives a  smoother staircase and sigmoid,  while a tempera(cid:173)\\nture  of  0  reduces  both  to  the  \\'\\'Hopfield\\'\\'  - like  threshold  function.  The  effects  of \\ntemperature  on  both  learning  and  recall  for  the  threshold  and  5  - state  activation \\noptions are discussed in section 4. \\n\\n4. LEARNING AND  RECALL  WITH VLSI  CONSTRAINTS \\n\\nBefore  implementing  the  reduced  - arithmetic  network  in  VLSI,  simulation  experi(cid:173)\\nments  were  conducted  to  verify  that  the  5  - state  model  represented  a  worthwhile \\nenhancement  over  simple  threshold  activation.  The  \"benchmark\"  problem  was \\nchosen  for  its  ubiquitousness,  rather  than  for  its  intrinsic  value.  The  implications \\nfor  learning  and  recall  of the  5  - state  model,  the  threshold  (2  - state)  model  and \\n- state)  were  compared  at  varying  temperatures \\nsmooth  sigmoidal  activation  (  00 \\nIn  each  simulation  a  totally \\nwith  a  restricted  dynamic  range  for  the  weights  Tij • \\ninterconnected  64  node  network  attempted  to  learn  32  random  patterns  using  the \\ndelta  rule  learning  algorithm  (see  for  example  [14]).  Each  pattern  was  then  cor(cid:173)\\nrupted  with  25%  noise  and  recall  attempted  to  probe  the  content  addressable \\nmemory properties under the three different activation options. \\nDuring  learning,  individual  weights  can  become  large  (positive  or  negative).  When \\nweights  are  \"driven\"  beyond  the  maximum  value  in  a  hardware  implementation, \\n\\n\\x0c579 \\n\\nwhich  is  determined  by  the  size  of  the  synaptic  weight  blocks,  some  limiting \\nmechanism  must  be  introduced.  For  example,  with  eight  bit  weight  registers,  the \\nlimitation is  -128  S  Tij  S  127.  With integer weights,  this can be seen to be a prob(cid:173)\\nlem  of  dynamic  range,  where  it  is  the  relationship  between  the  smallest  possible \\nweight  (± 1) and the largest  (+ 127/-128) that is the issue. \\nResults:  Fig.  6  shows  examples  of the  results  obtained,  studying  learning  using  5  -\\nstate  activation  at  different  temperatures,  and  recall  using  both  5  - state  and  thres(cid:173)\\nhold  activation.  At  temperature  T=O,  the  5  - state  and  threshold  models  are \\ndegenerate,  and  the results identical.  Increasing smoothness of activation  (tempera(cid:173)\\nture)  during  learning  improves  the  quality  of  learning  regardless  of  the  activation \\nfunction  used  in  recall,  as more patterns are recognised  successfully.  Using 5 - state \\nactivation  in recall  is more effective  than simple  threshold  activation.  The effect of \\ndynamic  range  restrictions  can  be  assessed  from  the  horizontal  axis,  where  T/j:6.  is \\nshown.  The results  from  these and  many  other experiments may  be  summarised  as \\nfollows:-\\n5 - State activation  vs.  threshold: \\n1)  Learning with 5  - state activation was  protracted  over the threshold  activation, \\nas  binary  patterns  were  being  learnt,  and  the  inclusion  of  intermediate  values \\nadded extra degrees of freedom. \\n\\n2)  Weight  sets  learnt  using  the  5  - state  activation  function  were  \"better\"  than \\nthose  learnt  via  threshold  activation,  as  the  recall  properties  of both  5  - state \\nand  threshold  networks  using  such  a  weight  set  were  more  robust  against \\nnoise. \\nFull  sigmoidal  activation  was  better  than  5  - state,  but  the  enhancement  was \\nless  significant  than  that  incurred  by  moving  from  threshold  - 5 - state.  This \\nsuggests  that the law  of diminishing returns  applies to  addition of levels to the \\nneural  state  Vi\\'  This  issue  has  been  studied  mathematically  [15],  with  results \\nthat agree  qualitatively with  ours. \\n\\n3) \\n\\nWeight Saturation: \\nThree  methods  were  tried  to  deal  with  weight  saturation.  Firstly,  inclusion  of  a \\ndecay,  or  \"forgetting\"  term  was  included  in  the  learning  cycle  [1].  It  is  our  view \\nthat  this  technique can  produce the desired weight limiting property,  but in  the time \\navailable  for  experiments,  we  were  unable  to  \"tune\"  the  rate  of  decay  sufficiently \\nwell  to  confirm  it.  Renormalisation  of the  weights  (division  to  bring large  weights \\nback  into  the  dynamic  range)  was  very  unsuccessful,  suggesting  that  information \\ndistributed  throughout  the  numerically small  weights  was  being  destroyed.  Finally, \\nthe  weights were  allowed  to  \"clip\"  (ie any weight  outside the dynamic range  was  set \\nto  the  maximum  allowed  value).  This method  proved  very  successful,  as  the learn(cid:173)\\ning  algorithm  adjusted the weights  over which  it still  had control  to  compensate for \\nthe  saturation effect.  It is  interesting to note  that  other experiments have indicated \\nthat  Hopfield  nets  can  \"forget\"  in a  different  way,  under different learning control, \\ngiving  preference  to  recently acquired  memories [16].  The results  from  the  satura(cid:173)\\ntion experiments were:-\\n1) \\n\\nFor  the  32  pattemJ64  node  problem,  integer  weights  with  a  dynamic  range \\ngreater than  ±30 were necessary to give enough  storage capability. \\nFor weights  with  maximum  values  TiJ  = 50-70,  \"clipping\"  occurs,  but  net(cid:173)\\nwork  performance  is  not  seriously  degraded  over  that  with  an  unrestricted \\nweight set. \\n\\n2) \\n\\n\\x0c580 \\n\\n15 \\n\\n\"0  10 \\nc = \\n.2 \\nen e u \\n5 --~ \\n\\n0 \\n\\n0 \\n\\nI \\n\\n\".\\' \\n\\n., ... \\n\\n.... ----------\\n\\n,-\\ne  ~ ;A ....... ;.. f:\\'-:\\' :::::7.:::.::-:::-: f\\'-. \\n,  ,. \\ni \\n! \\n! , \\ni \\nI \\nI , \\n\\n20  30 \\n\\n40  50  60  70 \\n\\nLimit \\n\\n15 \\n\\nT=30  _._.-.-\\nT=20 \\nT=10 \\nT=O \\n\\n-.-._.-.. \\n\\n,.. .•. -..... -.•. _ .•. \\n, \\n.. \\ni \\nj\\'\\'\\'\\'--\\n,,\\'i \\n\\n- . . .,. \\'\" \\n\\nj \\n\\n~-------------\\n••••••• •••••••••••••••• •••••• \\n\\nj \\nI \\n\\nO~~~~--~~ __ ~~ __ \\no \\n\\n20  30  40  50  60  70 \\n\\nLimit \\n\\n5 . state activation function  recal1 \\n\\ntlHopficld\" activation  function  recall \\n\\nFigure 6.  Recall  of patterns  learned  with  the  5  .  state  activation function  and  subse(cid:173)\\nquently restored using  the 5-state and the  hard - threshold activation functions. \\nT  is  the  \"temperature\",  or smoothness  of the  activation function,  and \"limit\"  the  value \\nofTI; ·  \\n\\nThese  results  showed  that  the  5  - state  model  was  worthy  of implementation  as  a \\nVLSI neural board, and suggested that 8 - bit weights were sufficient. \\n\\nS.  PROJECTED SPECIFICATION OF A HARDWARE NEURAL  BOARD \\n\\nThe specification of a  64  neuron board is  given  here,  using a  5 - state bit  - serial 64 \\nx 64  synapse array with  a derated clock speed  of 20 MHz.  The synaptic weights are \\n8  bit words and the word  length  of the running summation XI  is  16  bits to  allow for \\ngrowth.  A  64  synapse  column  has  a  computational  latency  of  80  clock  cycles  or \\nbits,  giving  an  update  time  of 4 .... s  for  the  network.  The  time  to  load  the  weights \\ninto  the  array  is  limited  to  6O .... s  by  the  supporting  RAM,  with  an  access  time  of \\n12Ons.  These  load  and  update  times  mean  that  the  network  is  executing  1  x  10\\' \\noperations/second,  where  one  operation  is  ±  Tlj  Vj •  This  is  much  faster  than  a \\nnatural  neural  network,  and  much  faster  than  is  necessary  in  a  hardware  accelera(cid:173)\\ntor.  We  have  therefore  developed  a  \"paging\"  architecture,  that  effectively  \"trades -\\noff\" some of this excessive speed against increased network size. \\nA  \"moving  - patch\"  neural  board:  An  array  of  the  5  - state  synapses  is  currently \\nbeing  fabricated  as  a  VLSI  integrated  circuit.  The  shift  registers  and \\nthe \\nadderlsubtractor for  each  synapse  occupy a  disappointingly large silicon  area,  allow(cid:173)\\ning only a  3  x 9 synaptic  array.  To achieve  a  suitable size  neural  network  from  this \\narray,  several chips need to be  included on a  board with  memory and control circu(cid:173)\\nitry.  The  \"moving  patch\"  concept  is  shown  in  figure  7,  where  a  small  array  of \\nsynapses is passed over a much larger n  x n  synaptic array. \\nEach  time  the  array  is  \"moved\"  to  represent  another set  of  synapses,  new  weights \\nmust be  loaded  into it.  For example,  the  first  set of weights will  be T 11  •. ,  T;J  ... T 21 \\n...  T 2j  to Tjj ,  the second  set  Tj + 1,l  to T u  etc..  The final  weight  to be loaded will  be \\n\\n\\x0c581 \\n\\nn  neurons .. om synaptic array \\n\\nSmaller \"Patch\" \\n\\nmoves over array \\n\\nrr~ _____ ) __ -.. \\n> \\n~\\'-\\n\\nFigure 7.  The  \"moving  patch\" concept,  passing  a  small synaptic \"patch\"  over  a larger \\nrun synapse array. \\n\\nTNt·  Static,  off - the  - shelf RAM is  used  to store the weights and the  whole opera(cid:173)\\ntion  is  pipelined for  maximum efficiency.  Figure 8 shows the board level design for \\nthe network. \\n\\nSynaptic  Accelerator Chips \\n\\nControl \\n\\nHOST \\nFigure 8. A  \"moving  patch\" neural network board. \\n\\nThe small  \"patch\" that moves  around  the array  to  give  n  neurons comprises 4 VLSI \\nsynaptic accelerator chips to give  a 6 x 18 synaptic array. The number of neurons to \\nbe  simulated  is 256  and  the weights for  these  are stored  in 0.5  Mb of RAM  with a \\nload  time  of 8ms.  For  each  \"patch\"  movement,  the  partial  runnin~ summatinn \\n\\n;. \\n\\n\\x0c582 \\n\\ncalculated  for  each  column,  is  stored  in  a  separate  RAM  until  it is  required  to  be \\nadded  into  the  next  appropriate  summation.  The  update  time  for  the  board  is  3ms \\ngiving  2  x  107  operations/second.  This  is  slower  than  the  64  neuron  specification, \\nbut  the  network  is  16  times  larger,  as  the  arithmetic  elements are  being  used  more \\nefficiently.  To  achieve  a  network  of  greater  than  256  neurons,  more  RAM  is \\nrequired to store the weights.  The network is then slower unless a larger number of \\naccelerator chips is  used  to give  a larger moving \"patch\". \\n\\n6.  CONCLUSIONS \\n\\nA  strategy  and  design  method  has  been  given  for  the  construction  of  bit  - serial \\nVLSI neural network chips and  circuit  boards.  Bit - serial  arithmetic,  coupled  to  a \\nreduced  arithmetic  style,  enhances  the  level  of  integration  possible  beyond  more \\nconventional digital,  bit - parallel schemes.  The restrictions imposed  on both synap(cid:173)\\ntic  weight  size  and  arithmetic  precision  by  VLSI  constraints  have  been  examined \\nand shown to be tolerable,  using the associative memory problem as a test. \\nWhile  we  believe  our  digital  approach  to  represent  a  good  compromise  between \\narithmetic  accuracy  and  circuit  complexity,  we  acknowledge  that  the  level  of \\nintegration  is  disappointingly  low. \\nIt  is  our  belief  that,  while  digital  approaches \\nmay  be interesting and  useful  in the medium  term,  essentially as  hardware accelera(cid:173)\\ntors for  neural simulations,  analog techniques represent the best  ultimate option in 2 \\n- dimensional  silicon.  To this  end,  we  are currently pursuing techniques for  analog \\nIn any  event,  the  full \\npseudo  - static  memory,  using  standard  CMOS  technology. \\ndevelopment  of a  nonvolatile  analog  memory  technology,  such  as  the  MNOS  tech(cid:173)\\nnique [7],  is key to the long - term  future of VLSI neural nets that can learn. \\n\\n7. ACKNOWLEDGEMENTS \\n\\nThe  authors  acknowledge  the  support  of  the  Science  and  Engineering  Research \\nCouncil (UK) in the execution of this work. \\n\\nReferences \\n\\n1. \\n\\nS.  Grossberg,  \"Some  Physiological  and  Biochemical  Consequences  of Psycho(cid:173)\\nlogical Postulates,\" Proc.  Natl.  Acad.  Sci.  USA,  vol.  60,  pp.  758  - 765,  1968. \\n\\n2.  H.  P.  Graf,  L.  D.  Jackel,  R.  E.  Howard,  B.  Straughn,  J.  S.  Denker,  W. \\nHubbard,  D.  M.  Tennant,  and  D.  Schwartz,  \"VLSI  Implementation  of  a \\nNeural  Network  Memory  with  Several  Hundreds  of  Neurons,\"  Proc.  AlP \\nConference on Neural Networks for  Computing.  Snowbird,  pp.  182 - 187,  1986. \\n3.  W.  S.  Mackie,  H.  P.  Graf,  and  J.  S.  Denker,  \"Microelectronic  Implementa(cid:173)\\n\\ntion  of  Connectionist  Neural  Network  Models,\"  IEEE  Conference  on  Neural \\nInformation Processing Systems.  Denver,  1987. \\nJ . J. Hopfield  and D.  W.  Tank, \"Neural\" Computation of Decisions in  Optim(cid:173)\\nisation Problems,\" BioI.  Cybern.,  vol.  52,  pp.  141  - 152,  1985. \\n\\n4. \\n\\n5.  M.  A.  Sivilotti,  M.  A.  Mahowald,  and  C.  A.  Mead, Real - Time  Visual Com(cid:173)\\n\\nputations Using  Analog CMOS  Processing Arrays, 1987.  To be published \\n\\n6.  C.  A.  Mead,  \"Networks  for  Real  - Time  Sensory  Processing,\"  IEEE  Confer(cid:173)\\n\\nence  on  Neural Information  Processing Systems,  Denver,  1987. \\n\\n\\x0c583 \\n\\n7. \\n\\n8. \\n\\nJ.  P.  Sage,  K.  Thompson.  and  R. S.  Withers,  \"An Artificial Neural  Network \\nIntegrated  Circuit  Based on MNOSlCCD  Principles,\"  Proc. AlP Conference on \\nNeural Networlcs for Computing,  Snowbird,  pp.  381  - 385,  1986. \\nS.  C.  J.  Garth, \"A Chipset for  High Speed  Simulation of Neural Network  Sys(cid:173)\\ntems,\"  IEEE Conference on Neural Networlc.s,  San Diego,  1987. \\n\\n9.  A.  F.  Murray and  A.  V.  W.  Smith,  \"A Novel  Computational  and  Signalling \\nMethod  for  VLSI Neural Networks,\"  European  Solid State Circuits Conference \\n, 1987. \\n\\n10.  A.  F.  Murray  and  A.  J.  W.  Smith,  \"Asynchronous  Arithmetic  for  VLSI \\n\\nNeural Systems,\"  Electronics Letters, vol.  23, no.  12, p.  642, June, 1987. \\n\\n11.  A.  F.  Murray  and  A.  V.  W.  Smith,  \"Asynchronous  VLSI  Neural  Networks \\n\\nusing  Pulse  Stream  Arithmetic,\"  IEEE  Journal  of Solid-State  Circuits  and Sys(cid:173)\\ntems,  1988.  To be published \\n\\n12.  M.  E.  Gaspar,  \"Pulsed  Neural  Networks:  Hardware,  Software  and  the  Hop(cid:173)\\nfield  AID  Converter  Example,\"  IEEE  Conference  on  Neural  Information  Pro(cid:173)\\ncessing Systems.  Denver,  1987. \\n\\n13.  M.  S.  McGregor,  P.  B.  Denyer,  and A.  F.  Murray,  \"A Single - Phase  Clock(cid:173)\\ning Scheme for  CMOS  VLSI,\"  Advanced Research  in  VLSI  \" Proceedings of the \\n1987 Stanford Conference,  1987. \\n\\n14.  D.  E.  Rumelhart,  G.  E.  Hinton,  and  R.  J.  Williams,  \"Learning  Internal \\nRepresentations  by  Error  Propagation,\"  Parallel  Distributed  Processing  \" \\nExplorations  in  the  Microstructure of Cognition,  vol.  1,  pp.  318 - 362,  1986. \\n\\n15.  M.  Fleisher  and  E.  Levin,  \"The  Hopfiled  Model  with  Multilevel  Neurons \\nModels,\"  IEEE  Conference  on  Neural  Information  Processing  Systems.  Denver, \\n1987. \\n\\n16.  G.  Parisi,  \"A  Memory  that  Forgets,\"  J.  Phys.  A  .\\'  Math.  Gen.,  vol.  19,  pp. \\n\\nL617  - L620,  1986. \\n\\n\\x0c',\n",
       " 'award': [],\n",
       " 'sourceid': 27,\n",
       " 'authors': [{'given_name': 'Alan',\n",
       "   'family_name': 'Murray',\n",
       "   'institution': None},\n",
       "  {'given_name': 'Anthony', 'family_name': 'Smith', 'institution': None},\n",
       "  {'given_name': 'Zoe', 'family_name': 'Butler', 'institution': None}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_url = \"https://papers.nips.cc/paper/1987/file/02e74f10e0327ad868d138f2b4fdd6f0-Metadata.json\"\n",
    "\n",
    "r = requests.get(ex_url)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69b8bcb5-c4f1-4433-b228-ff73cae9699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, NoneStr\n",
    "from typing import List, Tuple, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59775137-951d-498e-b0d2-28367800bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMl4Physics(BaseModel):\n",
    "    title: List[NoneStr] = []\n",
    "    authors: List[NoneStr] = []\n",
    "\n",
    "\n",
    "class HashYearDataFrame(BaseModel):\n",
    "    hash: List[NoneStr] = []\n",
    "    year: List[Optional[int]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "776a4c36-0bf6-403e-a919-10c4c7014678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['apple'], 'authors': []}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DataMl4Physics()\n",
    "data.title.append(\"apple\")\n",
    "data.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e5607c3-210e-48e2-a240-f5c74f5d195d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(\"https://papers.nips.cc/paper/2019/file/021e1ea77bd91aaa0fc4d01a943a654e-Bibtex.bib\") is str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3cea4bb-871b-42de-8f8b-790fd16c845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OperationFailure for auth failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d426540d-f2f3-4607-a47e-38141b6ef160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "type(Path()) is Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9430f940-00e3-4fd3-a8f0-6d9d2cf40ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'allow_dash',\n",
       " 'coerce_path_result',\n",
       " 'convert',\n",
       " 'dir_okay',\n",
       " 'envvar_list_splitter',\n",
       " 'exists',\n",
       " 'fail',\n",
       " 'file_okay',\n",
       " 'get_metavar',\n",
       " 'get_missing_message',\n",
       " 'is_composite',\n",
       " 'name',\n",
       " 'path_type',\n",
       " 'readable',\n",
       " 'resolve_path',\n",
       " 'split_envvar_value',\n",
       " 'type',\n",
       " 'writable']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import click\n",
    "dir(click.Path(path_type=Path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7fc13cd-3cff-4cd3-8e95-347e3c76767c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOOL',\n",
       " 'BadParameter',\n",
       " 'BoolParamType',\n",
       " 'Choice',\n",
       " 'CompositeParamType',\n",
       " 'DateTime',\n",
       " 'FLOAT',\n",
       " 'File',\n",
       " 'FloatParamType',\n",
       " 'FloatRange',\n",
       " 'FuncParamType',\n",
       " 'INT',\n",
       " 'IntParamType',\n",
       " 'IntRange',\n",
       " 'LazyFile',\n",
       " 'PY2',\n",
       " 'ParamType',\n",
       " 'Path',\n",
       " 'STRING',\n",
       " 'StringParamType',\n",
       " 'Tuple',\n",
       " 'UNPROCESSED',\n",
       " 'UUID',\n",
       " 'UUIDParameterType',\n",
       " 'UnprocessedParamType',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_get_argv_encoding',\n",
       " 'convert_type',\n",
       " 'datetime',\n",
       " 'filename_to_ui',\n",
       " 'get_filesystem_encoding',\n",
       " 'get_streerror',\n",
       " 'open_stream',\n",
       " 'os',\n",
       " 'safecall',\n",
       " 'stat',\n",
       " 'text_type']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(click.types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4505e8-74a9-41e5-be94-a7938bf9cdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb+srv://None:None@maincluster.otbuf.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\n"
     ]
    },
    {
     "ename": "OperationFailure",
     "evalue": "Authentication failed., full error: {'ok': 0, 'errmsg': 'Authentication failed.', 'code': 8000, 'codeName': 'AtlasError'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36m_get_socket\u001b[1;34m(self, all_credentials)\u001b[0m\n\u001b[0;32m   1277\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m                         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msockets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOperationFailure\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-201eb91849ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mexecute_before_any_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-201eb91849ce>\u001b[0m in \u001b[0;36mexecute_before_any_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_mongo_client\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmongo_username\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmongo_password\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"test_neurips\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mcollection\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_collection_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"collection\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\database.py\u001b[0m in \u001b[0;36mlist_collection_names\u001b[1;34m(self, session, filter, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m         return [result[\"name\"]\n\u001b[1;32m--> 863\u001b[1;33m                 for result in self.list_collections(session=session, **kwargs)]\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     def collection_names(self, include_system_collections=True,\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\database.py\u001b[0m in \u001b[0;36mlist_collections\u001b[1;34m(self, session, filter, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m                 **kwargs)\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m         return self.__client._retryable_read(\n\u001b[0m\u001b[0;32m    826\u001b[0m             _cmd, read_pref, session)\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\mongo_client.py\u001b[0m in \u001b[0;36m_retryable_read\u001b[1;34m(self, func, read_pref, session, address, retryable, exhaust)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretryable_reads_supported\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m                     \u001b[0mretryable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1464\u001b[1;33m                 with self._slaveok_for_server(read_pref, server, session,\n\u001b[0m\u001b[0;32m   1465\u001b[0m                                               \u001b[0mexhaust\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m                                                                    slave_ok):\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\mongo_client.py\u001b[0m in \u001b[0;36m_slaveok_for_server\u001b[1;34m(self, read_preference, server, session, exhaust)\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[0msingle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopology_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTOPOLOGY_TYPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSingle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1310\u001b[0m             slave_ok = (single and not sock_info.is_mongos) or (\n\u001b[0;32m   1311\u001b[0m                 read_preference != ReadPreference.PRIMARY)\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\mongo_client.py\u001b[0m in \u001b[0;36m_get_socket\u001b[1;34m(self, server, session, exhaust)\u001b[0m\n\u001b[0;32m   1244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_MongoClientErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr_handler\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m             with server.get_socket(\n\u001b[0m\u001b[0;32m   1247\u001b[0m                     self.__all_credentials, checkout=exhaust) as sock_info:\n\u001b[0;32m   1248\u001b[0m                 \u001b[0merr_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontribute_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36mget_socket\u001b[1;34m(self, all_credentials, checkout)\u001b[0m\n\u001b[0;32m   1229\u001b[0m             \u001b[0mlisteners\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpublish_connection_check_out_started\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1231\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_credentials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled_for_cmap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36m_get_socket\u001b[1;34m(self, all_credentials)\u001b[0m\n\u001b[0;32m   1279\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m                     \u001b[1;31m# Can raise ConnectionFailure or CertificateError.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1281\u001b[1;33m                     \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_credentials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1282\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_perished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self, all_credentials)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1197\u001b[1;33m             \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_auth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_credentials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1198\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m             \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConnectionClosedReason\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36mcheck_auth\u001b[1;34m(self, all_credentials)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcached\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mauthset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;31m# CMAP spec says to publish the ready event only after authenticating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36mauthenticate\u001b[1;34m(self, credentials)\u001b[0m\n\u001b[0;32m    808\u001b[0m           \u001b[1;33m-\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mMongoCredential\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m         \"\"\"\n\u001b[1;32m--> 810\u001b[1;33m         \u001b[0mauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;31m# negotiated_mechanisms are no longer needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\auth.py\u001b[0m in \u001b[0;36mauthenticate\u001b[1;34m(credentials, sock_info)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[0mmechanism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmechanism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[0mauth_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_AUTH_MAP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmechanism\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m     \u001b[0mauth_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\auth.py\u001b[0m in \u001b[0;36m_authenticate_default\u001b[1;34m(credentials, sock_info)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_authenticate_scram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SCRAM-SHA-256'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_authenticate_scram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SCRAM-SHA-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_wire_version\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_authenticate_scram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SCRAM-SHA-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\auth.py\u001b[0m in \u001b[0;36m_authenticate_scram\u001b[1;34m(credentials, sock_info, mechanism)\u001b[0m\n\u001b[0;32m    293\u001b[0m         nonce, first_bare, cmd = _authenticate_scram_start(\n\u001b[0;32m    294\u001b[0m             credentials, mechanism)\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[0mserver_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'payload'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36mcommand\u001b[1;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[0m\n\u001b[0;32m    681\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_if_not_writable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munacknowledged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m             return command(self, dbname, spec, slave_ok,\n\u001b[0m\u001b[0;32m    684\u001b[0m                            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_mongos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_preference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcodec_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m                            \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowable_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\network.py\u001b[0m in \u001b[0;36mcommand\u001b[1;34m(sock_info, dbname, spec, slave_ok, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, check_keys, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields, exhaust_allowed)\u001b[0m\n\u001b[0;32m    157\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                 helpers._check_command_response(\n\u001b[0m\u001b[0;32m    160\u001b[0m                     \u001b[0mresponse_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_wire_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowable_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                     parse_write_concern_error=parse_write_concern_error)\n",
      "\u001b[1;32mD:\\Users\\shadi\\anaconda3\\envs\\sandbox\\lib\\site-packages\\pymongo\\helpers.py\u001b[0m in \u001b[0;36m_check_command_response\u001b[1;34m(response, max_wire_version, allowable_errors, parse_write_concern_error)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mCursorNotFound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_wire_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mOperationFailure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_wire_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationFailure\u001b[0m: Authentication failed., full error: {'ok': 0, 'errmsg': 'Authentication failed.', 'code': 8000, 'codeName': 'AtlasError'}"
     ]
    }
   ],
   "source": [
    "def load_mongo_client(mongo_username: str, mongo_password: str) -> pymongo.MongoClient:\n",
    "    \"\"\"Loads MongoDB's client from Mongo credentials\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mongo_username : str\n",
    "        MongoDB's database username\n",
    "    mongo_password : str\n",
    "        MongoDB's database password\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pymongo.MongoClient\n",
    "        Database client of MongoDB\n",
    "    \"\"\"\n",
    "    mongo_uri = f\"mongodb+srv://{mongo_username}:{mongo_password}@maincluster.otbuf.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\"\n",
    "    print(mongo_uri)\n",
    "    client = pymongo.MongoClient(mongo_uri)\n",
    "    return client\n",
    "\n",
    "def execute_before_any_test():\n",
    "    mongo_username = os.environ.get(\"MONGO_USERNAME\")\n",
    "    mongo_password = os.environ.get(\"MONGO_PASSWORD\")\n",
    "    client = load_mongo_client(mongo_username, mongo_password)\n",
    "    db = client[\"test_neurips\"]\n",
    "    for collection in db.list_collection_names():\n",
    "        db[\"collection\"].drop()\n",
    "    client.close()\n",
    "    \n",
    "    \n",
    "execute_before_any_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee43442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
